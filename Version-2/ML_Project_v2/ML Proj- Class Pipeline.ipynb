{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcb9d80-a3fd-4cd6-829a-54a15c495678",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Mental_Stress_and_Coping_Mechanisms_processed_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. LOAD & PREPROCESS ‚Üí SPLIT ‚Üí SAVE\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMental_Stress_and_Coping_Mechanisms_processed_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the cleaned & feature-engineered CSV\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dummy_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStress Level Category_Low\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStress Level Category_Medium\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStress Level Category_High\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Mental_Stress_and_Coping_Mechanisms_processed_final.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. LOAD & PREPROCESS ‚Üí SPLIT ‚Üí SAVE\n",
    "# --------------------------------------------------\n",
    "df = pd.read_csv(\"Mental_Stress_and_Coping_Mechanisms_processed_final.csv\")\n",
    "\n",
    "\n",
    "# Load the cleaned & feature-engineered CSV\n",
    "dummy_cols = [\n",
    "    \"Stress Level Category_Low\",\n",
    "    \"Stress Level Category_Medium\",\n",
    "    \"Stress Level Category_High\"\n",
    "]\n",
    "df['Stress Level Category'] = (\n",
    "    df[dummy_cols]\n",
    "      .idxmax(axis=1)  # picks the dummy with a 1\n",
    "      .str.replace(\"Stress Level Category_\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# 3. NOW DROP those dummies and any other unwanted columns\n",
    "cols_to_drop = [\n",
    "    \"Mental Stress Level\",\n",
    "    *dummy_cols,\n",
    "    \"Stress Coping Mechanisms\",\n",
    "    \"Unnamed: 0\"\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# 4. ENGINEER FEATURES (unchanged)\n",
    "df['Gender_Other'] = (\n",
    "    df[['Gender_Agender','Gender_Bigender','Gender_Genderfluid']]\n",
    "      .sum(axis=1).clip(upper=1)\n",
    ")\n",
    "df[\"Stress_Ratio\"] = df[\"Financial Stress\"] / (df[\"Family Support\"] + 1e-5)\n",
    "df['Social_Media_Usage_per_week'] = df['Social Media Usage (Hours per day)'] * 7\n",
    "\n",
    "# 5. SELECT FEATURES & TARGET\n",
    "selected_features = [\n",
    "    'Age','Academic Performance (GPA)','Study Hours Per Week',\n",
    "    'Social_Media_Usage_per_week','Sleep Duration (Hours per night)',\n",
    "    'Physical Exercise (Hours per week)','Family Support','Financial Stress',\n",
    "    'Peer Pressure','Relationship Stress','Counseling Attendance','Diet Quality',\n",
    "    'Cognitive Distortions','Family Mental Health History','Medical Condition',\n",
    "    'Substance Use','Gender_Female','Gender_Male','Gender_Other','Stress_Ratio'\n",
    "]\n",
    "X = df[selected_features]\n",
    "y = df['Stress Level Category']\n",
    "\n",
    "# 6. OPTIONAL OUTLIER REMOVAL\n",
    "Q1, Q3 = X['Study Hours Per Week'].quantile([0.25,0.75])\n",
    "IQR = Q3 - Q1\n",
    "mask = X['Study Hours Per Week'].between(Q1-1.5*IQR, Q3+1.5*IQR)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# 7. SPLIT & SAVE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_df = X_train.copy()\n",
    "train_df['Stress Level Category'] = y_train\n",
    "test_df  = X_test.copy()\n",
    "test_df ['Stress Level Category'] = y_test\n",
    "\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df .to_csv(\"test_data.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5864ec85-4cd1-401d-9756-4159bc445cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "# 2. EXTRACT FEATURES & TARGET\n",
    "#    (Assumes 'Stress Level Category' is your target column)\n",
    "X_train_raw = train_df.drop(columns=[\"Stress Level Category\"])\n",
    "y_train_raw = train_df[\"Stress Level Category\"]\n",
    "\n",
    "# 3. LABEL‚ÄêMAP YOUR TARGET (optional, for consistent ordering)\n",
    "label_map = {'Low':0, 'Medium':1, 'High':2}\n",
    "y_train = y_train_raw.map(label_map).values\n",
    "\n",
    "# 4. STANDARDIZE FEATURES\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "# 5. BALANCE WITH SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba74964b-70b4-416d-937e-ad9a5bc833c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "üîß Best SVM parameters: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "üéØ Best CV accuracy   : 0.5975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Define the parameter grid for RBF-SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 2. Initialize a base SVM with balanced class weights and probability outputs\n",
    "svm_base = SVC(\n",
    "    class_weight='balanced',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Wrap it in GridSearchCV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=svm_base,\n",
    "    param_grid=svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Fit on the SMOTE‚Äêresampled training data\n",
    "svm_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 5. Pull out the best model and its CV score\n",
    "svm_best = svm_grid_search.best_estimator_\n",
    "svm_best_params = svm_grid_search.best_params_\n",
    "svm_best_score = svm_grid_search.best_score_\n",
    "\n",
    "print(\"üîß Best SVM parameters:\", svm_best_params)\n",
    "print(\"üéØ Best CV accuracy   :\", round(svm_best_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a35348d-95f2-46a8-b099-7b768d8455a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:15:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:15:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:15:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:15:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:15:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean CV Accuracy: 0.4587607323147938\n",
      "XGBoost CV Scores: [0.3871308  0.38080169 0.374868   0.468849   0.68215417]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the y labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train_resampled)\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "# Now cross-validation with encoded labels\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X_train_resampled, y_train_encoded, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"XGBoost Mean CV Accuracy:\", np.mean(xgb_cv_scores))\n",
    "print(\"XGBoost CV Scores:\", xgb_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7ae23b-253f-46a5-a1dd-80c0f47c2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [08:16:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.4802937546504841\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train_resampled)\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid_search.fit(X_train_resampled, y_train_encoded)\n",
    "\n",
    "# Results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee54cdde-7a2a-49cf-959d-63fb43936d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4049053417632408,\n",
       " 0.01828539153180415,\n",
       " array([0.39873418, 0.37447257, 0.40760296, 0.41393875, 0.42977825]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gb_cv_scores = cross_val_score(gb_model, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Output\n",
    "gb_mean_accuracy = np.mean(gb_cv_scores)\n",
    "gb_std_accuracy = np.std(gb_cv_scores)\n",
    "\n",
    "gb_mean_accuracy, gb_std_accuracy, gb_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a917fe61-8576-4162-8750-f04a8ffdb1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1,\n",
       "  'max_depth': 7,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 200},\n",
       " 0.46846893810790463)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "gb_grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=gb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform the grid search on training data\n",
    "gb_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Extract best parameters and score\n",
    "gb_best_params = gb_grid_search.best_params_\n",
    "gb_best_score = gb_grid_search.best_score_\n",
    "\n",
    "gb_best_params, gb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a6f3b0-37cd-4da2-bd18-64c525ea95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200},\n",
       " 0.44334763566046903)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gb_grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=gb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Extract best parameters and score\n",
    "gb_best_params = gb_grid_search.best_params_\n",
    "gb_best_score = gb_grid_search.best_score_\n",
    "\n",
    "gb_best_params, gb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208ddf0e-efdd-4822-ab1d-217031169198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3693\n",
      "[LightGBM] [Info] Number of data points in the train set: 3789, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3695\n",
      "[LightGBM] [Info] Number of data points in the train set: 3789, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3691\n",
      "[LightGBM] [Info] Number of data points in the train set: 3790, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098876\n",
      "[LightGBM] [Info] Start training from score -1.098876\n",
      "[LightGBM] [Info] Start training from score -1.098085\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3688\n",
      "[LightGBM] [Info] Number of data points in the train set: 3790, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098876\n",
      "[LightGBM] [Info] Start training from score -1.098085\n",
      "[LightGBM] [Info] Start training from score -1.098876\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3690\n",
      "[LightGBM] [Info] Number of data points in the train set: 3790, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098085\n",
      "[LightGBM] [Info] Start training from score -1.098876\n",
      "[LightGBM] [Info] Start training from score -1.098876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4483857529217293,\n",
       " 0.009893423236654708,\n",
       " array([0.43565401, 0.45780591, 0.46145723, 0.44033791, 0.44667371]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Train and evaluate LightGBM without hyperparameter tuning\n",
    "lgb_default_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgb_default_scores = cross_val_score(lgb_default_model, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Results\n",
    "lgb_default_mean = np.mean(lgb_default_scores)\n",
    "lgb_default_std = np.std(lgb_default_scores)\n",
    "\n",
    "lgb_default_mean, lgb_default_std, lgb_default_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41936ca1-2899-405a-bde4-be03ff9e7df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3718\n",
      "[LightGBM] [Info] Number of data points in the train set: 4737, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "‚úÖ Best LightGBM Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'min_child_samples': 10, 'n_estimators': 200, 'num_leaves': 31}\n",
      "‚úÖ Best CV Accuracy: 0.4539\n"
     ]
    }
   ],
   "source": [
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'num_leaves': [15, 31],\n",
    "    'min_child_samples': [10, 20]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "lgb_grid = GridSearchCV(\n",
    "    estimator=LGBMClassifier(random_state=42),\n",
    "    param_grid=lgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "lgb_grid.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and score\n",
    "lgb_best_params = lgb_grid.best_params_\n",
    "lgb_best_score = lgb_grid.best_score_\n",
    "\n",
    "print(\"‚úÖ Best LightGBM Parameters:\", lgb_best_params)\n",
    "print(\"‚úÖ Best CV Accuracy:\", round(lgb_best_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4320b10a-894d-446f-9915-170a58513435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4908215595328797,\n",
       " 0.014535322618368055,\n",
       " array([0.48523207, 0.47468354, 0.51531151, 0.49841605, 0.48046463]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Train and evaluate CatBoost without hyperparameter tuning\n",
    "cat_default_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_default_scores = cross_val_score(cat_default_model, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Results\n",
    "cat_default_mean = np.mean(cat_default_scores)\n",
    "cat_default_std = np.std(cat_default_scores)\n",
    "\n",
    "cat_default_mean, cat_default_std, cat_default_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85f3701-895d-45a6-8ead-3e04880cff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'depth': 7, 'iterations': 200, 'learning_rate': 0.1}, 0.4826046275379947)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for CatBoost\n",
    "cat_param_grid = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "cat_grid_search = GridSearchCV(\n",
    "    estimator=CatBoostClassifier(verbose=0, random_state=42),\n",
    "    param_grid=cat_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "cat_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and score\n",
    "cat_best_params = cat_grid_search.best_params_\n",
    "cat_best_score = cat_grid_search.best_score_\n",
    "\n",
    "cat_best_params, cat_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ab18c8-fbd8-4a7f-a8cf-aa5d1e40ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voting Ensemble Accuracy: 0.5858\n",
      "Fold-wise scores: [0.5665 0.5665 0.5839 0.6114 0.6008]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define your three tuned base-learners:\n",
    "rf_best = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=200,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cat_best = CatBoostClassifier(\n",
    "    depth=3,\n",
    "    iterations=100,\n",
    "    learning_rate=0.05,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_best = SVC(\n",
    "    kernel='rbf',\n",
    "    C=10,\n",
    "    gamma=1,\n",
    "    probability=True,    # <-- needed for soft voting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Build a soft-voting ensemble of all three:\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf',  rf_best),\n",
    "        ('cat', cat_best),\n",
    "        ('svm', svm_best)\n",
    "    ],\n",
    "    voting='soft'   # average their predicted probabilities\n",
    ")\n",
    "\n",
    "# 3. Evaluate with stratified 5-fold CV:\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(voting_model, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"‚úÖ Voting Ensemble Accuracy:\", round(np.mean(scores), 4))\n",
    "print(\"Fold-wise scores:\", np.round(scores, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3e5fdb3-4f88-4e79-9b4a-0e756e9aa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voting Classifier Accuracy:  0.463193\n",
      "Fold-wise: [0.38818565 0.39135021 0.40126716 0.46356917 0.67159451]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Best tuned models\n",
    "rf_best = RandomForestClassifier(\n",
    "    max_depth=10, n_estimators=200, min_samples_split=2,\n",
    "    min_samples_leaf=2, random_state=42\n",
    ")\n",
    "\n",
    "cat_best = CatBoostClassifier(\n",
    "    depth=3, iterations=100, learning_rate=0.05, verbose=0, random_state=42\n",
    ")\n",
    "\n",
    "# Combine them into Voting Classifier\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('rf', rf_best), ('cat', cat_best)],\n",
    "    voting='soft'  # 'hard' for majority class, 'soft' for probability average\n",
    ")\n",
    "\n",
    "# Evaluate using CV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(voting_model, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"‚úÖ Voting Classifier Accuracy: \", round(np.mean(scores), 6))\n",
    "print(\"Fold-wise:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0895aed8-f604-4130-ae32-831dd0fc2999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mean CV Accuracy: 0.4551\n",
      "‚úÖ Std Dev CV Accuracy: 0.0199\n",
      "‚úÖ Fold-wise Scores: [0.4188 0.4536 0.4572 0.4731 0.4731]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize HistGradientBoostingClassifier with custom hyperparameters\n",
    "hist_model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_iter=100,\n",
    "    max_leaf_nodes=31,\n",
    "    l2_regularization=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stratified 5-Fold cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "hist_scores = cross_val_score(hist_model, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Results\n",
    "mean_accuracy = np.mean(hist_scores)\n",
    "std_accuracy = np.std(hist_scores)\n",
    "\n",
    "print(f\"‚úÖ Mean CV Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"‚úÖ Std Dev CV Accuracy: {std_accuracy:.4f}\")\n",
    "print(f\"‚úÖ Fold-wise Scores: {np.round(hist_scores, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6668e9-76f7-476d-b3d1-7c536b0be476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5870889194836904,\n",
       " 0.01892239850284984,\n",
       " array([0.56329114, 0.57067511, 0.58394931, 0.61034847, 0.60718057]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Best tuned models from your results\n",
    "rf_best = RandomForestClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_best = SVC(\n",
    "    C=10,\n",
    "    gamma=1,\n",
    "    kernel='rbf',\n",
    "    probability=True,  # Needed for soft voting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# VotingClassifier setup (soft voting to combine probabilities)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_best), ('svm', svm_best)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Cross-validation with the resampled data\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "voting_scores = cross_val_score(voting_clf, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Results\n",
    "voting_mean_accuracy = np.mean(voting_scores)\n",
    "voting_std_accuracy = np.std(voting_scores)\n",
    "\n",
    "voting_mean_accuracy, voting_std_accuracy, voting_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c6e2a4a-ce1e-419e-ae29-63acba453143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def svm_grid_search_from_csv(\n",
    "    file_path: str,\n",
    "    target_column: str = \"Stress Level Category\",\n",
    "    label_map: dict = {'Low':0, 'Medium':1, 'High':2},\n",
    "    svm_params: dict = None,\n",
    "    cv: int = 5,\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    n_jobs: int = -1,\n",
    "    verbose: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load CSV, rebuild y if needed\n",
    "    2) Drop ID/text, map Yes/No ‚Üí 0/1\n",
    "    3) Split into train/test\n",
    "    4) Build ColumnTransformer: (impute+scale) for nums, (impute+OHE) for cats\n",
    "    5) Wrap that + SMOTE + SVC into an imblearn Pipeline\n",
    "    6) GridSearchCV over SVC hyper‚Äêparams\n",
    "    7) Print CV‚Äêtrain & hold‚Äêout test reports\n",
    "    8) Return the fitted pipeline + reports\n",
    "    \"\"\"\n",
    "    # 1) Load\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 1a) Reconstruct y if only one‚Äêhot cols exist\n",
    "    if target_column not in df.columns:\n",
    "        dummies = [c for c in df.columns if c.startswith(f\"{target_column}_\")]\n",
    "        df[target_column] = (\n",
    "            df[dummies].idxmax(axis=1)\n",
    "                       .str.replace(f\"{target_column}_\",\"\",regex=False)\n",
    "        )\n",
    "        df.drop(columns=dummies, inplace=True)\n",
    "\n",
    "    # 1b) Drop unusable text/ID cols\n",
    "    for c in (\"Stress Coping Mechanisms\",\"Student ID\",\"Unnamed: 0\"):\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=c, inplace=True)\n",
    "\n",
    "    # 1c) Map Yes/No ‚Üí 0/1\n",
    "    for yesno in (\"Counseling Attendance\",\n",
    "                  \"Family Mental Health History\",\n",
    "                  \"Medical Condition\"):\n",
    "        if yesno in df:\n",
    "            df[yesno] = df[yesno].map({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    # 2) X / y\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column].map(label_map).astype(int)\n",
    "\n",
    "    # 3) Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Identify numeric vs. categorical\n",
    "    num_feats = X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    cat_feats = X_train.select_dtypes(exclude=[\"int64\",\"float64\"]).columns.tolist()\n",
    "\n",
    "    # 5) Build preprocessing sub‚Äêpipelines\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "    preproc = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_feats),\n",
    "        (\"cat\", cat_pipe, cat_feats)\n",
    "    ])\n",
    "\n",
    "    # 6) Full pipeline: preprocessing ‚Üí SMOTE ‚Üí SVC\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"smote\", SMOTE(random_state=random_state)),\n",
    "        (\"svc\", SVC(class_weight=\"balanced\",\n",
    "                    probability=True,\n",
    "                    random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    # 7) Grid‚Äêsearch over only the SVC step\n",
    "    if svm_params is None:\n",
    "        svm_params = {\n",
    "            \"svc__C\":     [0.1, 1, 10, 100],\n",
    "            \"svc__gamma\": [\"scale\", 0.01, 0.1, 1],\n",
    "            \"svc__kernel\":[\"rbf\"]\n",
    "        }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=svm_params,\n",
    "        cv=StratifiedKFold(cv, shuffle=True, random_state=random_state),\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best = grid.best_estimator_\n",
    "    print(f\"üîß Best params: {grid.best_params_}\")\n",
    "    print(f\"üéØ CV accuracy: {grid.best_score_:.4f}\")\n",
    "\n",
    "    # 8a) Cross‚Äêvalidated TRAIN report\n",
    "    y_train_cv = cross_val_predict(\n",
    "        best, X_train, y_train,\n",
    "        cv=StratifiedKFold(cv, shuffle=True, random_state=random_state)\n",
    "    )\n",
    "    print(\"\\nüìä TRAIN (CV) Report:\")\n",
    "    print(classification_report(y_train, y_train_cv, target_names=list(label_map.keys())))\n",
    "    print(\"Confusion (train):\")\n",
    "    print(confusion_matrix(y_train, y_train_cv))\n",
    "\n",
    "    # 8b) HOLD‚ÄêOUT TEST report\n",
    "    y_test_pred = best.predict(X_test)\n",
    "    print(\"\\nüìâ TEST Report:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=list(label_map.keys())))\n",
    "    print(\"Confusion (test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    # 9) Return fitted pipeline + report dicts\n",
    "    train_rep = classification_report(\n",
    "        y_train, y_train_cv,\n",
    "        labels=[0,1,2],\n",
    "        target_names=list(label_map.keys()),\n",
    "        output_dict=True\n",
    "    )\n",
    "    test_rep  = classification_report(\n",
    "        y_test, y_test_pred,\n",
    "        labels=[0,1,2],\n",
    "        target_names=list(label_map.keys()),\n",
    "        output_dict=True\n",
    "    )\n",
    "    return best, grid.best_params_, train_rep, test_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13adff93-92b1-4347-9a68-b1138a8251b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "üîß Best params: {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': 'rbf'}\n",
      "üéØ CV accuracy: 0.3947\n",
      "\n",
      "üìä TRAIN (CV) Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      0.00      0.00       843\n",
      "      Medium       0.00      0.00      0.00       854\n",
      "        High       0.39      1.00      0.57      1105\n",
      "\n",
      "    accuracy                           0.39      2802\n",
      "   macro avg       0.46      0.33      0.19      2802\n",
      "weighted avg       0.46      0.39      0.22      2802\n",
      "\n",
      "Confusion (train):\n",
      "[[   1    1  841]\n",
      " [   0    0  854]\n",
      " [   0    0 1105]]\n",
      "\n",
      "üìâ TEST Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      0.01      0.01       362\n",
      "      Medium       1.00      0.01      0.02       366\n",
      "        High       0.40      1.00      0.57       474\n",
      "\n",
      "    accuracy                           0.40      1202\n",
      "   macro avg       0.80      0.34      0.20      1202\n",
      "weighted avg       0.76      0.40      0.23      1202\n",
      "\n",
      "Confusion (test):\n",
      "[[  2   0 360]\n",
      " [  0   3 363]\n",
      " [  0   0 474]]\n",
      "üîß Best params: {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': 'rbf'}\n",
      "\n",
      "üìä TRAIN (CV) report:\n",
      "\n",
      "              precision    recall  f1-score      support\n",
      "Low            1.000000  0.001186  0.002370   843.000000\n",
      "Medium         0.000000  0.000000  0.000000   854.000000\n",
      "High           0.394643  1.000000  0.565941  1105.000000\n",
      "accuracy       0.394718  0.394718  0.394718     0.394718\n",
      "macro avg      0.464881  0.333729  0.189437  2802.000000\n",
      "weighted avg   0.456488  0.394718  0.223898  2802.000000\n",
      "\n",
      "üìâ TEST (hold‚Äêout) report:\n",
      "\n",
      "              precision    recall  f1-score      support\n",
      "Low            1.000000  0.005525  0.010989   362.000000\n",
      "Medium         1.000000  0.008197  0.016260   366.000000\n",
      "High           0.395990  1.000000  0.567325   474.000000\n",
      "accuracy       0.398502  0.398502  0.398502     0.398502\n",
      "macro avg      0.798663  0.337907  0.198191  1202.000000\n",
      "weighted avg   0.761813  0.398502  0.231981  1202.000000\n"
     ]
    }
   ],
   "source": [
    "# 1) Fit on train_data.csv\n",
    "best_model, best_params, train_metrics, test_metrics = \\\n",
    "    svm_grid_search_from_csv(\"train_data.csv\")\n",
    "\n",
    "# 2) Display what was returned\n",
    "print(\"üîß Best params:\", best_params)\n",
    "print(\"\\nüìä TRAIN (CV) report:\\n\")\n",
    "print(pd.DataFrame(train_metrics).T)\n",
    "\n",
    "print(\"\\nüìâ TEST (hold‚Äêout) report:\\n\")\n",
    "print(pd.DataFrame(test_metrics).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff6301ba-209c-4791-98c3-c4e1c424cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Persisted scaler.joblib, svm_model.joblib, label_map.json, feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "# ‚Ä¶ your existing imports ‚Ä¶\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# 1) ‚Ä¶ after you‚Äôve fit & selected best_model and scaler ‚Ä¶\n",
    "\n",
    "# Persist the StandardScaler\n",
    "joblib.dump(scaler,     \"scaler.joblib\")\n",
    "\n",
    "# Persist the tuned SVM\n",
    "joblib.dump(best_model, \"svm_model.joblib\")\n",
    "\n",
    "# Persist the label‚Äêmap (int‚Üístring)\n",
    "label_map = {'Low':0, 'Medium':1, 'High':2}\n",
    "inv_map   = {str(v): k for k,v in label_map.items()}\n",
    "with open(\"label_map.json\",\"w\") as f:\n",
    "    json.dump(inv_map, f)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# NEW: Persist the *exact* feature order you trained on\n",
    "# Make sure `feature_columns` is the list of the columns\n",
    "# you used for training X (in the same order).\n",
    "# If you built X via `X = df[selected_features]`,\n",
    "# simply re‚Äêdump that list here:\n",
    "\n",
    "feature_columns = [\n",
    "    'Age',\n",
    "    'Academic Performance (GPA)',\n",
    "    'Study Hours Per Week',\n",
    "    'Social_Media_Usage_per_week',\n",
    "    'Sleep Duration (Hours per night)',\n",
    "    'Physical Exercise (Hours per week)',\n",
    "    'Family Support',\n",
    "    'Financial Stress',\n",
    "    'Peer Pressure',\n",
    "    'Relationship Stress',\n",
    "    'Counseling Attendance',\n",
    "    'Diet Quality',\n",
    "    'Cognitive Distortions',\n",
    "    'Family Mental Health History',\n",
    "    'Medical Condition',\n",
    "    'Substance Use',\n",
    "    'Gender_Female',\n",
    "    'Gender_Male',\n",
    "    'Gender_Other',\n",
    "    'Stress_Ratio'\n",
    "]\n",
    "\n",
    "with open(\"feature_columns.json\",\"w\") as f:\n",
    "    json.dump(feature_columns, f)\n",
    "\n",
    "print(\"‚úÖ Persisted scaler.joblib, svm_model.joblib, label_map.json, feature_columns.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
